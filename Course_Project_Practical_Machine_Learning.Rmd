---
title: "Course Project - Practical Machine Learning"
author: "*Presented by* c2a0s14*, for* \n Practical Machine Learning *Class with Coursera*"
output: html_document
---

### Introduction

This report describes and displays the method I used to do prediction modeling using the exercise data from the groupware web site (cited below).  When the data was created, the researchers recorded data regarding 'how' the exercise was performed.  The outcome of each action was recorded as the 'classe' in the data.  The purpose of this project is to build a Machine Learning Algorithm to predict the manner in which participants did the exercise (i.e. predict the 'classe' for each row). 

### Initial Setup: Libraries, Parallel Processing Options, and Functions

```{r setupLibraries, message=FALSE, warning=FALSE, results='hide'}
rm(list=ls())
libs <- sapply(c('knitr','caret', 'randomForest', 'doParallel'), library, quietly=TRUE, character.only=TRUE)
registerDoParallel(cores=2)
```

```{r setupFunctions}
getfile <- function(filename, outfilename, forceit=FALSE) {
    if(!file.exists(outfilename) | forceit == TRUE)
        download.file(filename, outfilename, mode="wb")
}  
```

### Get, split, and clean the training data

The data for this project come from http://groupware.les.inf.puc-rio.br/har.  It contains 19622 observations with 160 columns each.  The first seven columns are 'bookkeeping' columns which contain data such as who did the exercise, timestamps, num_window and new_window.  The dataset contains a particular quirk because the researchers recorded summary data for particular 'windows' of exercises performed.  They recorded the summary data on rows where new_window="yes".  In the cases where new_window="no", there is no summary data recorded; thus, leaving many features with no values recorded.  Specifically, this left 19216 rows with NAs in each of the 100 summary data columns.  You will see below that the 7 bookkeeping features along with the 100 features that are 97% NAs were deleted before creating the prediction model.

```{r getTrainingData}
# Use this training file for building models
getfile("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", "pml-training.csv")
pmltraining <- read.csv("pml-training.csv", na.strings=c('NA','#DIV/0!',' ',''))
```

```{r splitTrainingData}
# Split the training dataset - Reserve 30% to use for out-of-sample cross validation
set.seed(424298)
InTrain <- createDataPartition(pmltraining$classe, p=0.7, list=FALSE)
training <- pmltraining[InTrain,]
testing <- pmltraining[-InTrain,]
```

```{r cleanTrainingData}
# Look at the data in 100% of training.  Remove unnecessary columns in the 70% portion.
removal.index <- c()                  #Create index of columns to remove from training dataset
removal.index <-  cbind(removal.index, c(1:7))    #Prepare to remove bookkeeping features
for (i in 1:length(pmltraining)) {                #Prepare to remove features that are mostly NAs
    if (sum(is.na(pmltraining[,i])) >= 19216)     #Many columns have 19,216 NAs or higher (97%)
        removal.index <- rbind(removal.index, i)  #All 100 of these features are all NAs in test.
}
training <- training[,-removal.index]  #Remove columns that are not needed from the 70% portion.
dim(training)                          #Show new training data with only 53 columns remaining
```

### Build and Train the Machine Learning Algorithm

I chose to build the Machine Learning Algorithm using caret's 'train' function using the 'random forest' method.  Random Forest has a reputation for being very accurate especially for datasets with many interacting features (which perfectly describes this dataset.)  I chose to use 'cv' with 'number'=5 to do 5-fold cross-validation thus reducing the possibility of overfitting.  This function automatically chooses the  model with the highest accuracy as the final model.

```{r doTrainRandomForestCrossValidationModelFit}
# Train model using caret's 'train' with 'random forest' method and 5-fold cross-validation.
my_cvfit_model_file <- "my_cvfit_model.Rds"
if (file.exists(my_cvfit_model_file)) {
    cvFit <- readRDS(my_cvfit_model_file)
    } else {
    set.seed(12345)
    fitControl <- trainControl(method="cv", number=5, repeats=4)
    cvFit <- train(classe ~ ., data=training, method="rf", 
               trControl=fitControl, model=FALSE, 
               seeds=c(22344, 43322), allowParallel=TRUE)
    saveRDS(cvFit, my_cvfit_model_file)
    }
cvFit
cvFit$finalModel
```

### Do Cross Validation to get Out-Of-Sample Error Estimates

Cross-validation was used on the 70% portion of the dataset to create the final model above.  The "OOB estimate of error rate" was .74%.  This is the "in-sample" error rate since it was calculated using the same data that was used to pick the predictors and build the model. To get the estimated "out-of-sample" error rate, here I cross-validate against the independent data (the 30% portion).  As expected, we see the **"out-of-sample" error rate of .78%** is slightly higher than the **"in-sample" error rate of .74%**.

```{r doOutOfSampleErrorRateCalculation}
# Do cross validation testing on 30% of training dataset that was reserved for testing.
pred.cvtest <- predict(cvFit, newdata=testing)
(sum(pred.cvtest != testing$classe)) / nrow(testing)  # Show out-of-sample error rate
table(pred.cvtest, testing$classe)
```

### Conclusion

Using the Random Forest method with 5-fold cross validation created a highly accurate Machine Learning Algorithm with over 99% accuracy when applied to the testing dataset.


### Appendix A - Submit Answers for Auto-Graded Portion of Assignment

```{r getTestingData}
# Use this function to create files for the auto-graded submission
pml_write_files = function(x){
    for (i in 1:length(x)){
        filename = paste0("problem_id_",i,".txt")
        write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE) }
}

# Use this file for auto-graded submission ONLY
getfile("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",  "pml-testing.csv")
pmltesting  <- read.csv("pml-testing.csv") 

# Do prediction and create files for submission portion of the assignment
answers <- as.character(predict(cvFit, newdata=pmltesting))
pml_write_files(answers)
```
